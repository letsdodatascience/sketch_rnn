{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributions as dist\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(\"sketchrnn-cat.full.npz\", encoding='latin1')\n",
    "data = dataset['test']\n",
    "data = purify(data)\n",
    "data = normalize(data)\n",
    "Nmax = max_size(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size=256,\n",
    "                 z_size=128,\n",
    "                 dropout=0.9):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.z_size = z_size\n",
    "        self.rnn = nn.GRU(5, self.hidden_size, dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(2*self.hidden_size, self.z_size)\n",
    "        self.fc_logvar = nn.Linear(2*self.hidden_size, self.z_size)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        _, hidden = self.rnn(inputs)\n",
    "        hidden_cat = torch.cat(hidden.split(1,0),2).squeeze(0)\n",
    "        mu = self.fc_mu(hidden_cat)\n",
    "        logvar = self.fc_logvar(hidden_cat)\n",
    "        return mu,logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size=256,\n",
    "                 z_size=128,\n",
    "                 dropout=0.9):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.rnn = nn.GRU(z_size+5, hidden_size, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(z_size,hidden_size)\n",
    "        \n",
    "    def init_state(self,z):\n",
    "        return F.tanh(self.fc(z).unsqueeze(0))\n",
    "    \n",
    "    def forward(self,inputs,z):\n",
    "        hidden = self.init_state(z)\n",
    "        zs = z.unsqueeze(1).expand(-1,inputs.size(1),-1)\n",
    "        inputs_z = torch.cat([inputs,zs],dim=2)\n",
    "        return self.rnn(inputs_z,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hidden_size=256,\n",
    "                 dec_hidden_size=256,\n",
    "                 z_size=128,\n",
    "                 dropout=0.9,\n",
    "                 M=20):\n",
    "        super(SketchRNN, self).__init__()\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.M = M\n",
    "        self.encoder = Encoder(enc_hidden_size,z_size,dropout)\n",
    "        self.decoder = Decoder(dec_hidden_size,z_size,dropout)\n",
    "        self.gmm = nn.Linear(dec_hidden_size,6*M+3)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        enc_inputs = inputs[:,1:,:]\n",
    "        mu,logvar = self.encoder(enc_inputs)\n",
    "        z = self.reparameterize(mu,logvar)\n",
    "        \n",
    "        dec_inputs = inputs[:,:-1,:]\n",
    "        outputs,_ = self.decoder(dec_inputs,z)\n",
    "        outputs = self.gmm(outputs.contiguous().view(-1,self.dec_hidden_size))\n",
    "        o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_pen, o_pen_logits = self.get_mixture_coef(outputs)\n",
    "        \n",
    "        return z,o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_pen, o_pen_logits\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def get_mixture_coef(self,outputs):\n",
    "        z_pen_logits = outputs[:, 0:3]  # pen states\n",
    "        z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr = torch.split(outputs[:, 3:], self.M, 1)\n",
    "        # process output z's into MDN paramters\n",
    "\n",
    "        # softmax all the pi's and pen states:\n",
    "        z_pi = F.softmax(z_pi,1)\n",
    "        z_pen = F.softmax(z_pen_logits,1)\n",
    "\n",
    "        # exponentiate the sigmas and also make corr between -1 and 1.\n",
    "        z_sigma1 = torch.exp(z_sigma1)\n",
    "        z_sigma2 = torch.exp(z_sigma2)\n",
    "        z_corr = F.tanh(z_corr)\n",
    "\n",
    "        r = [z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_pen, z_pen_logits]\n",
    "        return r\n",
    "    \n",
    "    def get_lossfunc(self,z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr,\n",
    "                     z_pen_logits, x1_data, x2_data, pen_data):\n",
    "        \"\"\"Returns a loss fn based on eq #26 of http://arxiv.org/abs/1308.0850.\"\"\"\n",
    "        # This represents the L_R only (i.e. does not include the KL loss term).\n",
    "        \n",
    "        def normal_2d(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "            \"\"\"Returns result of eq # 24 of http://arxiv.org/abs/1308.0850.\"\"\"\n",
    "            norm1 = x1 - mu1\n",
    "            norm2 = x2 - mu2\n",
    "            s1s2 = s1*s2\n",
    "            # eq 25\n",
    "            z = norm1.div(s1).pow(2) + norm2.div(s2).pow(2) - 2*(rho*norm1*norm2).div(s1s2)\n",
    "            neg_rho = 1 - rho.pow(2)\n",
    "            result = (-z).div(2*neg_rho).exp()\n",
    "            denom = 2 * np.pi * s1s2 * torch.sqrt(neg_rho)\n",
    "            result = result.div(denom)\n",
    "            return result\n",
    "        \n",
    "        result0 = normal_2d(x1_data, x2_data, z_mu1, z_mu2, z_sigma1, z_sigma2,z_corr)\n",
    "        epsilon = 1e-6\n",
    "        # result1 is the loss wrt pen offset (L_s in equation 9 of\n",
    "        # https://arxiv.org/pdf/1704.03477.pdf)\n",
    "        result1 = result0*z_pi\n",
    "        result1 = torch.sum(result1, 1, keepdim=True)\n",
    "        result1 = -torch.log(result1 + epsilon)  # avoid log(0)\n",
    "\n",
    "        fs = 1.0 - pen_data[:, 2]  # use training data for this\n",
    "        fs = fs.view(-1, 1)\n",
    "      # Zero out loss terms beyond N_s, the last actual stroke\n",
    "        result1 = result1*fs\n",
    "        \n",
    "        _,labels = pen_data.max(1)\n",
    "        result2 = F.cross_entropy(z_pen_logits,labels,reduce=False)\n",
    "        if not self.training:\n",
    "            result2 = result2 * fs\n",
    "        \n",
    "        result = result1 + result2.unsqueeze(1)\n",
    "        return result.mean()\n",
    "    \n",
    "    def KLD(self,mu,logvar):\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    def generate(self,seq_len,z):\n",
    "        sample = Variable(torch.Tensor([0,0,1,0,0]),requires_grad=False).view(1,1,5)\n",
    "        hidden = self.decoder.init_state(z)\n",
    "        \n",
    "        def bivariate_normal(m1,m2,s1,s2,rho,temp=1.0):\n",
    "            mean = [m1, m2]\n",
    "            s1 *= temp * temp\n",
    "            s2 *= temp * temp\n",
    "            cov = [[s1 * s1, rho * s1 * s2], [rho * s1 * s2, s2 * s2]]\n",
    "            x = np.random.multivariate_normal(mean, cov, 1,check_valid='ignore')\n",
    "            return x[0][0], x[0][1]\n",
    "        \n",
    "        for _ in range(seq_len):\n",
    "            inputs = sample[:,-1:,:]\n",
    "            inputs_z = torch.cat([inputs,z.unsqueeze(1)],dim=2)\n",
    "            outputs,hidden = self.decoder.rnn(inputs_z,hidden)\n",
    "            outputs = self.gmm(outputs.contiguous().view(-1,self.dec_hidden_size))\n",
    "            o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_pen, o_pen_logits = self.get_mixture_coef(outputs)\n",
    "            \n",
    "            state = Variable(torch.zeros(1,1,5),requires_grad=False)\n",
    "            pen_status = dist.Categorical(o_pen.view(-1)).sample()\n",
    "            state[:,:,pen_status.data[0]+2] = 1\n",
    "            \n",
    "            M_id = dist.Categorical(o_pi.view(-1)).sample().data[0]\n",
    "            mu_1 = o_mu1[0,M_id].data\n",
    "            mu_2 = o_mu2[0,M_id].data\n",
    "            sigma_1 = o_sigma1[0,M_id].data\n",
    "            sigma_2 = o_sigma2[0,M_id].data\n",
    "            o_corr = o_corr[0,M_id].data\n",
    "           \n",
    "            x,y = bivariate_normal(mu_1,mu_2,sigma_1,sigma_2,o_corr)\n",
    "            state[:,:,0] = x\n",
    "            state[:,:,1] = y\n",
    "            \n",
    "            sample = torch.cat([sample,state],dim=1)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch = Sketch(data)\n",
    "training_data = DataLoader(sketch ,batch_size=100, shuffle=True,collate_fn=collate_fn_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SketchRNN()\n",
    "train_step = optim.Adam(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.57416605949\n",
      "2.41275024414\n",
      "2.38926815987\n",
      "2.32114386559\n",
      "2.33202314377\n",
      "2.28989601135\n",
      "2.16976857185\n",
      "2.0873708725\n",
      "2.07914400101\n",
      "2.11441731453\n",
      "1.90477955341\n",
      "1.89647293091\n",
      "1.87478649616\n",
      "1.84154009819\n",
      "1.70401978493\n",
      "1.65748381615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-621ba846e811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                                       o_corr,o_pen_logits, x1_data, x2_data, pen_data)\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mrec_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mrec_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/odie/anaconda/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/odie/anaconda/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epcoch in range(10):\n",
    "    for data in training_data:\n",
    "        data = Variable(data)\n",
    "        z, o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_pen, o_pen_logits = model(data)\n",
    "        \n",
    "        target = data[:,1:,:]\n",
    "        target = target.contiguous().view(-1,5)\n",
    "        x1_data, x2_data, eos_data, eoc_data, cont_data = target.split(1,1)\n",
    "        pen_data = torch.cat([eos_data, eoc_data, cont_data], 1)\n",
    "        rec_loss = model.get_lossfunc(o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, \n",
    "                                      o_corr,o_pen_logits, x1_data, x2_data, pen_data)\n",
    "        train_step.zero_grad()\n",
    "        rec_loss.backward()\n",
    "        train_step.step()\n",
    "        print rec_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = model.generate(100,z[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = next(iter(sketch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = collate_fn_([d1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.cumsum(d1.numpy()[:,0])\n",
    "y = np.cumsum(d1.numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,d1.size(0)):\n",
    "    if not d1.numpy()[i,2]:\n",
    "        plt.plot(x[i:i+2],-y[i:i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.cumsum(d2.numpy()[:,0])\n",
    "y = np.cumsum(d2.numpy()[:,1])\n",
    "\n",
    "for i in range(1,d2.size(0)):\n",
    "    if d2.numpy()[i,2]:\n",
    "        plt.plot(x[i:i+2],-y[i:i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
